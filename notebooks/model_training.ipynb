{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3a6f37ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "import cv2\n",
    "import json\n",
    "from PIL import Image\n",
    "\n",
    "class BDDDetectionDataset(Dataset):\n",
    "    def __init__(self, image_dir, annotation_file, category_map=None, transforms=None, evaluation=False):\n",
    "        self.image_dir = image_dir\n",
    "        self.transforms = transforms\n",
    "        self.evaluation = evaluation\n",
    "\n",
    "        with open(annotation_file) as f:\n",
    "            self.annotations = json.load(f)\n",
    "\n",
    "        self.category_map = category_map or self._generate_category_map()\n",
    "        self.image_annotations = self._organize_annotations()\n",
    "\n",
    "    def _generate_category_map(self):\n",
    "        # Extract all categories from the annotation file and assign integer labels\n",
    "        categories = set()\n",
    "        for item in self.annotations:\n",
    "            for label in item.get('labels', []):\n",
    "                categories.add(label['category'])\n",
    "        categories = sorted(list(categories))\n",
    "        categories.remove(\"lane\")\n",
    "        categories.remove(\"drivable area\")\n",
    "        return {cat: idx + 1 for idx, cat in enumerate(categories)}  # +1 because 0 is background\n",
    "\n",
    "    def _organize_annotations(self):\n",
    "        image_annots = []\n",
    "        for item in self.annotations:\n",
    "            filename = item['name']\n",
    "            labels = item.get('labels', [])\n",
    "\n",
    "            boxes = []\n",
    "            labels_idx = []\n",
    "\n",
    "            for label in labels:\n",
    "                if 'box2d' not in label:\n",
    "                    continue\n",
    "                box = label['box2d']\n",
    "                x1, y1, x2, y2 = box['x1'], box['y1'], box['x2'], box['y2']\n",
    "                \n",
    "                if x2 <= x1 or y2 <= y1:\n",
    "                    continue\n",
    "                boxes.append([x1, y1, x2, y2])\n",
    "                labels_idx.append(self.category_map[label['category']])\n",
    "            \n",
    "            # Skip image with no valid boxes\n",
    "            if len(boxes) == 0:\n",
    "                continue  \n",
    "                \n",
    "            image_annots.append({\n",
    "                'filename': filename,\n",
    "                'boxes': boxes,\n",
    "                'labels': labels_idx\n",
    "            })\n",
    "\n",
    "        return image_annots\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data = self.image_annotations[idx]\n",
    "        img_path = os.path.join(self.image_dir, data['filename'])\n",
    "\n",
    "\n",
    "\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "        boxes = torch.tensor(data['boxes'], dtype=torch.float32)\n",
    "        labels = torch.tensor(data['labels'], dtype=torch.int64)\n",
    "        target = {\n",
    "            'boxes': boxes,\n",
    "            'labels': labels,\n",
    "        }\n",
    "        if self.evaluation:\n",
    "            target['image_id'] = data['filename']\n",
    "\n",
    "        if self.transforms:\n",
    "            img = self.transforms(img)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_annotations)\n",
    "\n",
    "    def get_category_map(self):\n",
    "        return self.category_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3a28cb0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "transform_list = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "train_image_dir = \"/home/kkp3kor/2025/bdk_object_detection/data/bdd100k_images_100k/bdd100k/images/100k/train\"\n",
    "train_label_json = \"/home/kkp3kor/2025/bdk_object_detection/data/bdd100k_labels_release/bdd100k/labels/bdd100k_labels_images_train.json\"\n",
    "train_dataset = BDDDetectionDataset(\n",
    "    train_image_dir, \n",
    "    train_label_json, \n",
    "    category_map=None, \n",
    "    transforms=transform_list,\n",
    "    evaluation=False)\n",
    "train_data_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, collate_fn=lambda x: tuple(zip(*x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e242c325",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bike': 1,\n",
       " 'bus': 2,\n",
       " 'car': 3,\n",
       " 'motor': 4,\n",
       " 'person': 5,\n",
       " 'rider': 6,\n",
       " 'traffic light': 7,\n",
       " 'traffic sign': 8,\n",
       " 'train': 9,\n",
       " 'truck': 10}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.category_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c1ef68e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_image_dir = \"/home/kkp3kor/2025/bdk_object_detection/data/bdd100k_images_100k/bdd100k/images/100k/val\"\n",
    "valid_label_json = \"/home/kkp3kor/2025/bdk_object_detection/data/bdd100k_labels_release/bdd100k/labels/bdd100k_labels_images_val.json\"\n",
    "valid_dataset = BDDDetectionDataset(\n",
    "    valid_image_dir, \n",
    "    valid_label_json, \n",
    "    category_map=train_dataset.category_map, \n",
    "    transforms=transform_list,\n",
    "    evaluation=True)\n",
    "valid_data_loader = DataLoader(valid_dataset, batch_size=16, shuffle=True, collate_fn=lambda x: tuple(zip(*x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "49b30f58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_classes = len(train_dataset.category_map) + 1  # +1 for background class\n",
    "num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0037d211",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FasterRCNN(\n",
       "  (transform): GeneralizedRCNNTransform(\n",
       "      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
       "      Resize(min_size=(800,), max_size=1333, mode='bilinear')\n",
       "  )\n",
       "  (backbone): BackboneWithFPN(\n",
       "    (body): IntermediateLayerGetter(\n",
       "      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "      (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "      (layer1): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer2): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer3): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (4): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (5): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer4): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (fpn): FeaturePyramidNetwork(\n",
       "      (inner_blocks): ModuleList(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (2): Conv2dNormActivation(\n",
       "          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (3): Conv2dNormActivation(\n",
       "          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (layer_blocks): ModuleList(\n",
       "        (0-3): 4 x Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (extra_blocks): LastLevelMaxPool()\n",
       "    )\n",
       "  )\n",
       "  (rpn): RegionProposalNetwork(\n",
       "    (anchor_generator): AnchorGenerator()\n",
       "    (head): RPNHead(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (cls_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (bbox_pred): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (roi_heads): RoIHeads(\n",
       "    (box_roi_pool): MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=(7, 7), sampling_ratio=2)\n",
       "    (box_head): TwoMLPHead(\n",
       "      (fc6): Linear(in_features=12544, out_features=1024, bias=True)\n",
       "      (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    )\n",
       "    (box_predictor): FastRCNNPredictor(\n",
       "      (cls_score): Linear(in_features=1024, out_features=11, bias=True)\n",
       "      (bbox_pred): Linear(in_features=1024, out_features=44, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "import torch\n",
    "\n",
    "model = fasterrcnn_resnet50_fpn(pretrained=\"/home/kkp3kor/.cache/torch/hub/checkpoints/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\")\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4d5b997f",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "model.train()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.005, momentum=0.9, weight_decay=0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "862ab41b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ce61fd8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "def train_model(model, train_loader, val_loader, optimizer, device, num_epochs=100, save_path=\"model/best_model.pth\"):\n",
    "    best_val_loss = float('inf')\n",
    "    model.to(device)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "\n",
    "        for images, targets in train_loader:\n",
    "            images = [img.to(device) for img in images]\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "            loss_dict = model(images, targets)\n",
    "            losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            losses.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += losses.item()\n",
    "\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "\n",
    "        # ------------------ Validation ------------------\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for images, targets in val_loader:\n",
    "                images = [img.to(device) for img in images]\n",
    "                targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "                loss_dict = model(images, targets)\n",
    "                losses = sum(loss for loss in loss_dict.values())\n",
    "                val_loss += losses.item()\n",
    "\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}] - Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "        # Save best model\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "            print(f\"✅ Saved best model at epoch {epoch+1} with val loss {best_val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d97b5ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100] - Train Loss: 0.8761 | Val Loss: 0.8395\n",
      "✅ Saved best model at epoch 1 with val loss 0.8395\n",
      "Epoch [2/100] - Train Loss: 0.8141 | Val Loss: 0.8161\n",
      "✅ Saved best model at epoch 2 with val loss 0.8161\n",
      "Epoch [3/100] - Train Loss: 0.7982 | Val Loss: 0.8084\n",
      "✅ Saved best model at epoch 3 with val loss 0.8084\n",
      "Epoch [4/100] - Train Loss: 0.7887 | Val Loss: 0.8097\n",
      "Epoch [5/100] - Train Loss: 0.7827 | Val Loss: 0.8093\n",
      "Epoch [6/100] - Train Loss: 0.7786 | Val Loss: 0.8119\n"
     ]
    }
   ],
   "source": [
    "train_model(model, train_data_loader, valid_data_loader, optimizer, device, num_epochs=100, save_path=\"model/best_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "093d36ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(\"model/best_model.pth\"))\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "def run_inference(model, dataloader, device, score_thresh=0.4, save_dir=\"outputs/vis\"):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    predictions = []\n",
    "    i = 0\n",
    "    for images, targets in tqdm(dataloader):\n",
    "        images = [img.to(device) for img in images]\n",
    "        with torch.no_grad():\n",
    "            outputs = model(images)\n",
    "\n",
    "        for image, output, target in zip(images, outputs, targets):\n",
    "            keep = output[\"scores\"] > score_thresh\n",
    "            pred_boxes = output[\"boxes\"][keep].cpu()\n",
    "            pred_labels = output[\"labels\"][keep].cpu()\n",
    "            pred_scores = output[\"scores\"][keep].cpu()\n",
    "\n",
    "            predictions.append({\n",
    "                \"filename\": target[\"image_id\"],\n",
    "                \"pred_boxes\": pred_boxes,\n",
    "                \"pred_scores\": pred_scores,\n",
    "                \"gt_boxes\": target[\"boxes\"],\n",
    "                \"pred_labels\": pred_labels,\n",
    "                \"gt_labels\": target[\"labels\"]\n",
    "            })\n",
    "        if i == 400:\n",
    "            break\n",
    "        i += 1\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0381c018",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 400/625 [02:17<01:17,  2.90it/s]\n"
     ]
    }
   ],
   "source": [
    "predictions = run_inference(model, valid_data_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "aa8489f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.ops import box_iou\n",
    "\n",
    "def compute_eval_metrics(predictions, iou_thresh=0.5):\n",
    "    TP, FP, FN = 0, 0, 0\n",
    "    for pred in predictions:\n",
    "        preds = pred[\"pred_boxes\"]\n",
    "        gts = pred[\"gt_boxes\"]\n",
    "        if len(preds) == 0 and len(gts) == 0:\n",
    "            continue\n",
    "        elif len(preds) == 0:\n",
    "            FN += len(gts)\n",
    "            continue\n",
    "        elif len(gts) == 0:\n",
    "            FP += len(preds)\n",
    "            continue\n",
    "        ious = box_iou(preds, gts)\n",
    "        matched = (ious.max(dim=1).values > iou_thresh).sum().item()\n",
    "        TP += matched\n",
    "        FP += len(preds) - matched\n",
    "        FN += len(gts) - matched\n",
    "\n",
    "    precision = TP / (TP + FP + 1e-6)\n",
    "    recall = TP / (TP + FN + 1e-6)\n",
    "    f1 = 2 * (precision * recall) / (precision + recall + 1e-6)\n",
    "    return precision, recall, f1\n",
    "\n",
    "\n",
    "import json\n",
    "import os\n",
    "\n",
    "def save_predictions_to_json(predictions, save_path=\"outputs/predictions.json\"):\n",
    "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "\n",
    "    def convert(obj):\n",
    "        if isinstance(obj, torch.Tensor):\n",
    "            return obj.tolist()\n",
    "        elif isinstance(obj, dict):\n",
    "            return {k: convert(v) for k, v in obj.items()}\n",
    "        elif isinstance(obj, list):\n",
    "            return [convert(v) for v in obj]\n",
    "        else:\n",
    "            return obj\n",
    "\n",
    "    clean_preds = convert(predictions)\n",
    "\n",
    "    with open(save_path, \"w\") as f:\n",
    "        json.dump(clean_preds, f, indent=2)\n",
    "\n",
    "import json\n",
    "import torch\n",
    "\n",
    "def load_predictions_from_json(json_path):\n",
    "    with open(json_path, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    for pred in data:\n",
    "        # Convert lists back to tensors\n",
    "        pred[\"pred_boxes\"] = torch.tensor(pred[\"pred_boxes\"], dtype=torch.float32)\n",
    "        pred[\"pred_labels\"] = torch.tensor(pred[\"pred_labels\"], dtype=torch.int64)\n",
    "        pred[\"gt_boxes\"] = torch.tensor(pred[\"gt_boxes\"], dtype=torch.float32)\n",
    "        pred[\"gt_labels\"] = torch.tensor(pred[\"gt_labels\"], dtype=torch.int64)\n",
    "        pred[\"pred_scores\"] = torch.tensor(pred[\"pred_scores\"], dtype=torch.int64)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f5aa35b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.634 | Recall: 0.797 | F1 Score: 0.706\n",
      "Precision: 0.634 | Recall: 0.797 | F1 Score: 0.706\n"
     ]
    }
   ],
   "source": [
    "#predictions = run_inference(model, valid_data_loader, device)\n",
    "precision, recall, f1 = compute_eval_metrics(predictions)\n",
    "print(f\"Precision: {precision:.3f} | Recall: {recall:.3f} | F1 Score: {f1:.3f}\")\n",
    "save_predictions_to_json(predictions, \"outputs/predictions.json\")\n",
    "pred_data = load_predictions_from_json(\"outputs/predictions.json\")\n",
    "precision, recall, f1 = compute_eval_metrics(pred_data)\n",
    "print(f\"Precision: {precision:.3f} | Recall: {recall:.3f} | F1 Score: {f1:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "400112a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.utils import draw_bounding_boxes\n",
    "import torchvision.transforms.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualize_sample(image_tensor, gt_boxes, pred_boxes, pred_labels, save_path):\n",
    "    img = (image_tensor * 255).byte().clone()\n",
    "    img = draw_bounding_boxes(img, gt_boxes, colors=\"green\", labels=[\"GT\"] * len(gt_boxes))\n",
    "    img = draw_bounding_boxes(img, pred_boxes, colors=\"red\", labels=[\"PRED\"] * len(pred_boxes))\n",
    "    img = F.to_pil_image(img)\n",
    "    img.save(save_path)\n",
    "    \n",
    "i = 0\n",
    "for pred in predictions:\n",
    "    img_path = os.path.join(valid_image_dir, pred[\"filename\"])\n",
    "    if os.path.exists(img_path):\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        image_tensor = F.to_tensor(image)\n",
    "        visualize_sample(image_tensor, pred[\"gt_boxes\"], pred[\"pred_boxes\"], pred[\"pred_labels\"],\n",
    "                         f\"outputs/vis/{pred['filename']}\")\n",
    "    if i == 10:\n",
    "        break\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e14fd58e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import average_precision_score\n",
    "import torch\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from torchvision.ops import box_iou\n",
    "\n",
    "def compute_classwise_ap(predictions, num_classes, iou_thresh=0.5):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        predictions: list of dicts with keys: pred_boxes, pred_labels, pred_scores, gt_boxes, gt_labels\n",
    "        num_classes: total number of classes (excluding background=0)\n",
    "        iou_thresh: IoU threshold for positive match\n",
    "    Returns:\n",
    "        ap_per_class: dict with class_id -> AP\n",
    "        mAP: mean AP across classes\n",
    "    \"\"\"\n",
    "\n",
    "    # Collect all predictions and ground truth info\n",
    "    pred_by_class = defaultdict(list)\n",
    "    gt_by_class = defaultdict(list)\n",
    "\n",
    "    for pred in predictions:\n",
    "        pred_boxes = pred[\"pred_boxes\"]\n",
    "        pred_labels = pred[\"pred_labels\"]\n",
    "        pred_scores = pred[\"pred_scores\"]\n",
    "        gt_boxes = pred[\"gt_boxes\"]\n",
    "        gt_labels = pred[\"gt_labels\"]\n",
    "\n",
    "        for c in range(1, num_classes + 1):\n",
    "            # Filter for current class\n",
    "            cls_pred_mask = pred_labels == c\n",
    "            cls_gt_mask = gt_labels == c\n",
    "\n",
    "            cls_pred_boxes = pred_boxes[cls_pred_mask]\n",
    "            cls_pred_scores = pred_scores[cls_pred_mask]\n",
    "            cls_gt_boxes = gt_boxes[cls_gt_mask]\n",
    "\n",
    "            matched = torch.zeros(len(cls_gt_boxes))  # track matched GT boxes\n",
    "            tp = torch.zeros(len(cls_pred_boxes))\n",
    "            fp = torch.zeros(len(cls_pred_boxes))\n",
    "\n",
    "            for i, pbox in enumerate(cls_pred_boxes):\n",
    "                if len(cls_gt_boxes) == 0:\n",
    "                    fp[i] = 1\n",
    "                    continue\n",
    "                ious = box_iou(pbox.unsqueeze(0), cls_gt_boxes)[0]\n",
    "                max_iou, max_idx = ious.max(0)\n",
    "\n",
    "                if max_iou >= iou_thresh and matched[max_idx] == 0:\n",
    "                    tp[i] = 1\n",
    "                    matched[max_idx] = 1\n",
    "                else:\n",
    "                    fp[i] = 1\n",
    "\n",
    "            # Store per-class results\n",
    "            pred_by_class[c].extend(zip(cls_pred_scores.tolist(), tp.tolist(), fp.tolist()))\n",
    "            gt_by_class[c].append(len(cls_gt_boxes))\n",
    "\n",
    "    # Compute precision-recall curve & AP for each class\n",
    "    ap_per_class = {}\n",
    "    for c in range(1, num_classes + 1):\n",
    "        if len(pred_by_class[c]) == 0:\n",
    "            ap_per_class[c] = 0.0\n",
    "            continue\n",
    "\n",
    "        pred_by_class[c].sort(key=lambda x: -x[0])  # sort by score descending\n",
    "        scores, tps, fps = zip(*pred_by_class[c])\n",
    "\n",
    "        scores = np.array(scores)\n",
    "        tps = np.array(tps)\n",
    "        fps = np.array(fps)\n",
    "\n",
    "        tps_cum = np.cumsum(tps)\n",
    "        fps_cum = np.cumsum(fps)\n",
    "        recalls = tps_cum / (sum(gt_by_class[c]) + 1e-6)\n",
    "        precisions = tps_cum / (tps_cum + fps_cum + 1e-6)\n",
    "\n",
    "        # Interpolate precision\n",
    "        ap = 0.0\n",
    "        for r in np.linspace(0, 1, 11):\n",
    "            p = precisions[recalls >= r].max() if np.any(recalls >= r) else 0\n",
    "            ap += p / 11.0\n",
    "        ap_per_class[c] = round(ap, 4)\n",
    "\n",
    "    mAP = round(np.mean(list(ap_per_class.values())), 4)\n",
    "    return ap_per_class, mAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "25300125",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ mAP@[IoU=0.4]: 0.4770\n",
      "📊 Class-wise AP:\n",
      "bike                : 0.3993\n",
      "bus                 : 0.4823\n",
      "car                 : 0.7835\n",
      "motor               : 0.3261\n",
      "person              : 0.5886\n",
      "rider               : 0.3483\n",
      "traffic light       : 0.6685\n",
      "traffic sign        : 0.6375\n",
      "train               : 0.0000\n",
      "truck               : 0.5357\n"
     ]
    }
   ],
   "source": [
    "num_classes = len(train_dataset.category_map)  # e.g., 10\n",
    "ap_per_class, mean_ap = compute_classwise_ap(predictions, num_classes, iou_thresh=0.4)\n",
    "\n",
    "print(f\"\\n✅ mAP@[IoU=0.4]: {mean_ap:.4f}\")\n",
    "print(\"📊 Class-wise AP:\")\n",
    "for class_id, ap in ap_per_class.items():\n",
    "    class_name = [k for k, v in train_dataset.category_map.items() if v == class_id][0]\n",
    "    print(f\"{class_name:20}: {ap:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "630238d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ mAP@[IoU=0.5]: 0.4395\n",
      "📊 Class-wise AP:\n",
      "bike                : 0.3797\n",
      "bus                 : 0.4807\n",
      "car                 : 0.7011\n",
      "motor               : 0.3217\n",
      "person              : 0.5103\n",
      "rider               : 0.3420\n",
      "traffic light       : 0.5576\n",
      "traffic sign        : 0.6226\n",
      "train               : 0.0000\n",
      "truck               : 0.4797\n"
     ]
    }
   ],
   "source": [
    "ap_per_class, mean_ap = compute_classwise_ap(predictions, num_classes, iou_thresh=0.5)\n",
    "\n",
    "print(f\"\\n✅ mAP@[IoU=0.5]: {mean_ap:.4f}\")\n",
    "print(\"📊 Class-wise AP:\")\n",
    "for class_id, ap in ap_per_class.items():\n",
    "    class_name = [k for k, v in train_dataset.category_map.items() if v == class_id][0]\n",
    "    print(f\"{class_name:20}: {ap:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "68b533ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ mAP@[IoU=0.5]: 0.3861\n",
      "📊 Class-wise AP:\n",
      "bike                : 0.2900\n",
      "bus                 : 0.4221\n",
      "car                 : 0.6673\n",
      "motor               : 0.2440\n",
      "person              : 0.4793\n",
      "rider               : 0.3331\n",
      "traffic light       : 0.4101\n",
      "traffic sign        : 0.5445\n",
      "train               : 0.0000\n",
      "truck               : 0.4702\n"
     ]
    }
   ],
   "source": [
    "ap_per_class, mean_ap = compute_classwise_ap(predictions, num_classes, iou_thresh=0.6)\n",
    "\n",
    "print(f\"\\n✅ mAP@[IoU=0.5]: {mean_ap:.4f}\")\n",
    "print(\"📊 Class-wise AP:\")\n",
    "for class_id, ap in ap_per_class.items():\n",
    "    class_name = [k for k, v in train_dataset.category_map.items() if v == class_id][0]\n",
    "    print(f\"{class_name:20}: {ap:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9371c858",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test_torch",
   "language": "python",
   "name": "my_poetry_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
