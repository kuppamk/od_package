# ğŸš— BDD100K Object Detection Pipeline (Faster R-CNN, PyTorch)

This project implements an end-to-end object detection pipeline using the **BDD100K dataset** and **Faster R-CNN (ResNet50)**. It includes:

- ğŸ“¦ Custom dataset handler for BDD100K annotations
- ğŸ‹ï¸â€â™‚ï¸ Training and validation pipeline
- ğŸ§  Inference with confidence thresholding
- ğŸ“¸ Bounding box visualization after inference
- ğŸ“Š Evaluation using mAP and IoU metrics
- ğŸ“ JSON-based prediction saving
- ğŸ› ï¸ YAML-based config file
- ğŸ“ƒ Integrated logging to console and file

---

## ğŸ“ Project Structure

bdk_object_detection/ â”‚ â”œâ”€â”€ od_model/ â”‚ â”œâ”€â”€ bdd_dataset.py # Dataset and DataLoader setup â”‚ â”œâ”€â”€ config.py # YAML loader for config â”‚ â”œâ”€â”€ config.yaml # All config constants and hyperparameters â”‚ â”œâ”€â”€ evaluate.py # Evaluator with mAP, precision, recall â”‚ â”œâ”€â”€ inference.py # Inference and prediction saving â”‚ â”œâ”€â”€ model.py # Faster R-CNN model loader â”‚ â”œâ”€â”€ pipelines.py # Training, inference, eval, visualization pipelines â”‚ â”œâ”€â”€ train.py # Training loop â”‚ â”œâ”€â”€ utils.py # JSON helpers (save/load predictions) â”‚ â”œâ”€â”€ visualize.py # Visualize GT and predicted boxes â”‚ â”œâ”€â”€ main.py # Entry script with CLI support â”œâ”€â”€ artifacts/ # Saved model, logs, prediction JSON, visual outputs â”œâ”€â”€ requirements.txt # Python dependencies â””â”€â”€ README.md # Project documentation

---

## ğŸ› ï¸ Setup

### âœ… 1. Clone the Repository

```bash
git clone https://github.com/your-username/bdk_object_detection.git
cd bdk_object_detection
```

### âœ… 2.  Install Dependencies
```bash
pip install -r requirements.txt
```

### âœ… 3. Prepare the Dataset

Download the **BDD100K images and labels** from the official website:

- **Images**: [https://bdd-data.berkeley.edu/](https://bdd-data.berkeley.edu/)
- **Labels**: [https://bdd-data.berkeley.edu/](https://bdd-data.berkeley.edu/)

After downloading and extracting, update the paths in `od_model/config.yaml:`:

```yaml
TRAIN_IMAGE_DIR: "/path/to/bdd100k/images/100k/train"
TRAIN_LABEL_JSON_PATH:  "/path/to/bdd100k_labels_images_train.json"
VALID_IMAGE_DIR: "/path/to/bdd100k/images/100k/val"
VALID_LABEL_JSON_PATH:  "/path/to/bdd100k_labels_images_val.json"
BASE_WEIGTHS:  "/path/to/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth"
```

## ğŸš€ Running the Project

The project supports a CLI interface via argparse.

### ğŸ”§ Available Flags
--train â€” Run the training pipeline

--inference â€” Run inference using a trained model

--eval â€” Evaluate predictions using metrics like mAP, Precision, Recall

--visualize â€” Visualize predicted and ground-truth bounding boxes

--model_path â€” Path to the .pth model file

--config_path â€” (Optional) Path to your YAML config file (default: config.yaml)

### ğŸ§ª Example Commands

```bash
# Run training, inference, and evaluation
python main.py --train --inference --eval --visualize --model_path artifacts/best_model.pth

# Run inference only
python main.py --inference --model_path artifacts/best_model.pth

# Run evaluation only
python main.py --eval

# Visualization only
python main.py --visualize
```

## âš™ï¸ Execution Modes & Requirements

Below are detailed instructions and prerequisites for each pipeline mode:

---

### ğŸ‹ï¸â€â™‚ï¸ `--train`: Train the Model

Runs the full training loop and saves the best model checkpoint.

#### âœ… Requirements

- Make sure paths to training and validation images/labels are correctly set in:
  - `od_model/config.yaml`

#### ğŸ”„ What it does

- Loads the BDD100K dataset
- Trains the model using Faster R-CNN model
- Saves the best model to `artifacts/best_model.pth`
- Logs progress to artifacts/run.log

#### ğŸ’» Command

```bash
python main.py --train
```

### ğŸ§  `--inference`: Run Inference

Performs inference on the validation set and saves predictions.

#### âœ… Requirements

- Trained model `.pth` file (e.g., `artifacts/best_model.pth`)
- Dataset must be available (image + label paths in `od_model/config.py`)
- Set `--model_path` to your `.pth` file

#### ğŸ”„ What it does

- Loads the model and validation images
- Runs inference with a score threshold (configurable in `config.py`)
- Saves predictions to `artifacts/predictions.json`

#### ğŸ’» Command

```bash
python main.py --inference --model_path artifacts/best_model.pth
```

### ğŸ“Š `--eval`: Evaluate Model Performance

Evaluates saved predictions using mAP, precision, recall, and per-class AP.

#### âœ… Requirements

- `artifacts/predictions.json` must exist (generated from a previous inference run)
- Number of classes must match the model output (configured in `pipelines.py`)

#### ğŸ”„ What it does

- Loads predictions from the saved JSON file
- Computes the following metrics:
  - Overall **Precision**, **Recall**, **F1 Score**
  - **mAP** (mean Average Precision) across multiple IoU thresholds
  - **Class-wise Average Precision (AP)**

#### ğŸ’» Command

```bash
python main.py --eval
```
### ğŸ–¼ï¸ `--visualize`: Visualize Predictions

Saves side-by-side visualization of predicted and GT bounding boxes.

#### âœ… Requirements

- `artifacts/predictions.json` must exist (generated from inference)
- Validation images must be available and paths set in `config.yaml`

#### ğŸ”„ What it does

- Overlays bounding boxes on each image:
  - ğŸŸ© Ground Truth (green boxes)
  - ğŸŸ¥ Predictions (red boxes)
- Saves visualizations to `artifacts/vis/` as `.jpg` images

#### ğŸ’» Command

```bash
python main.py --visualize
```

## ğŸ“Œ Features

- ğŸ”„ **Modular & reusable architecture**
- ğŸ§© **Easy YAML-based configuration**
- ğŸªµ **Logging** to both terminal and file (`artifacts/run.log`)
- ğŸ” **Visualization support** after inference
- ğŸ§  **Supports both CPU & CUDA**

---

## ğŸ§¼ TODOs & Improvements
- [ ] Add support for COCO-style evaluation metrics
- [ ] Add early stopping and LR scheduler


